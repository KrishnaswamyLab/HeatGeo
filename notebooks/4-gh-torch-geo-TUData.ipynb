{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ChebConv\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "import pygsp\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.embedding import EmbHeatGeo\n",
    "from src.mds import embed_MDS\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: MUTAG(188):\n",
      "====================\n",
      "Number of graphs: 188\n",
      "Number of features: 7\n",
      "Number of classes: 2\n",
      "\n",
      "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 17\n",
      "Number of edges: 38\n",
      "Average node degree: 2.24\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 150\n",
      "Number of test graphs: 38\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12345)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:150]\n",
    "test_dataset = dataset[150:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 2636], x=[1188, 7], edge_attr=[2636, 4], y=[64], batch=[1188], ptr=[65])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 2506], x=[1139, 7], edge_attr=[2506, 4], y=[64], batch=[1139], ptr=[65])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 22\n",
      "DataBatch(edge_index=[2, 852], x=[387, 7], edge_attr=[852, 4], y=[22], batch=[387], ptr=[23])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(7, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 006, Train Acc: 0.6533, Test Acc: 0.7368\n",
      "Epoch: 007, Train Acc: 0.7467, Test Acc: 0.7632\n",
      "Epoch: 008, Train Acc: 0.7267, Test Acc: 0.7632\n",
      "Epoch: 009, Train Acc: 0.7200, Test Acc: 0.7632\n",
      "Epoch: 010, Train Acc: 0.7133, Test Acc: 0.7895\n",
      "Epoch: 011, Train Acc: 0.7200, Test Acc: 0.7632\n",
      "Epoch: 012, Train Acc: 0.7200, Test Acc: 0.7895\n",
      "Epoch: 013, Train Acc: 0.7200, Test Acc: 0.7895\n",
      "Epoch: 014, Train Acc: 0.7133, Test Acc: 0.8421\n",
      "Epoch: 015, Train Acc: 0.7133, Test Acc: 0.8421\n",
      "Epoch: 016, Train Acc: 0.7533, Test Acc: 0.7368\n",
      "Epoch: 017, Train Acc: 0.7400, Test Acc: 0.7632\n",
      "Epoch: 018, Train Acc: 0.7133, Test Acc: 0.8421\n",
      "Epoch: 019, Train Acc: 0.7400, Test Acc: 0.7895\n",
      "Epoch: 020, Train Acc: 0.7533, Test Acc: 0.7368\n",
      "Epoch: 021, Train Acc: 0.7467, Test Acc: 0.7895\n",
      "Epoch: 022, Train Acc: 0.7467, Test Acc: 0.7895\n",
      "Epoch: 023, Train Acc: 0.7533, Test Acc: 0.7895\n",
      "Epoch: 024, Train Acc: 0.7267, Test Acc: 0.8421\n",
      "Epoch: 025, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 026, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 027, Train Acc: 0.7600, Test Acc: 0.8158\n",
      "Epoch: 028, Train Acc: 0.7533, Test Acc: 0.8421\n",
      "Epoch: 029, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 030, Train Acc: 0.7600, Test Acc: 0.8158\n",
      "Epoch: 031, Train Acc: 0.7600, Test Acc: 0.8158\n",
      "Epoch: 032, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 033, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 034, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 035, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 036, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 037, Train Acc: 0.7400, Test Acc: 0.7632\n",
      "Epoch: 038, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 039, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 040, Train Acc: 0.7533, Test Acc: 0.7368\n",
      "Epoch: 041, Train Acc: 0.7467, Test Acc: 0.7368\n",
      "Epoch: 042, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 043, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 044, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 045, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 046, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 047, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 048, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 049, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 050, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 051, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 052, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 053, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 054, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 055, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 056, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 057, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 058, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 059, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 060, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 061, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 062, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 063, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 064, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 065, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 066, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 067, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 068, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 069, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 070, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 071, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 072, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 073, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 074, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 075, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 076, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 077, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 078, Train Acc: 0.7733, Test Acc: 0.8421\n",
      "Epoch: 079, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 080, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 081, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 082, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 083, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 084, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 085, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 086, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 087, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 088, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 089, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 090, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 091, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 092, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 093, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 094, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 095, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 096, Train Acc: 0.7600, Test Acc: 0.7895\n",
      "Epoch: 097, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 098, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 099, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 100, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 101, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 102, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 103, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 104, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 105, Train Acc: 0.7733, Test Acc: 0.7368\n",
      "Epoch: 106, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 107, Train Acc: 0.7733, Test Acc: 0.7105\n",
      "Epoch: 108, Train Acc: 0.8000, Test Acc: 0.7632\n",
      "Epoch: 109, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 110, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 111, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 112, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 113, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 114, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 115, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 116, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 117, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 118, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 119, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 120, Train Acc: 0.8000, Test Acc: 0.7105\n",
      "Epoch: 121, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 122, Train Acc: 0.7667, Test Acc: 0.7105\n",
      "Epoch: 123, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 124, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 125, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 126, Train Acc: 0.7733, Test Acc: 0.7368\n",
      "Epoch: 127, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 128, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 129, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 130, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 131, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 132, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 133, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 134, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 135, Train Acc: 0.8067, Test Acc: 0.7368\n",
      "Epoch: 136, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 137, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 138, Train Acc: 0.8133, Test Acc: 0.7105\n",
      "Epoch: 139, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 140, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 141, Train Acc: 0.8000, Test Acc: 0.6579\n",
      "Epoch: 142, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 143, Train Acc: 0.7933, Test Acc: 0.7632\n",
      "Epoch: 144, Train Acc: 0.7867, Test Acc: 0.7368\n",
      "Epoch: 145, Train Acc: 0.8267, Test Acc: 0.7368\n",
      "Epoch: 146, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 147, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 148, Train Acc: 0.7933, Test Acc: 0.7895\n",
      "Epoch: 149, Train Acc: 0.8200, Test Acc: 0.7105\n",
      "Epoch: 150, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 151, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 152, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 153, Train Acc: 0.8067, Test Acc: 0.7368\n",
      "Epoch: 154, Train Acc: 0.8067, Test Acc: 0.7368\n",
      "Epoch: 155, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 156, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 157, Train Acc: 0.8000, Test Acc: 0.7368\n",
      "Epoch: 158, Train Acc: 0.7800, Test Acc: 0.7368\n",
      "Epoch: 159, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 160, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 161, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 162, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 163, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 164, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 165, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 166, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 167, Train Acc: 0.7867, Test Acc: 0.7895\n",
      "Epoch: 168, Train Acc: 0.7867, Test Acc: 0.7895\n",
      "Epoch: 169, Train Acc: 0.8000, Test Acc: 0.7632\n",
      "Epoch: 170, Train Acc: 0.8000, Test Acc: 0.7632\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_heat_feature(loaders, tau_min=0.5, tau_max=100, n_tau=5, lap_type=\"normalized\", order=30):\n",
    "    for dataloader in loaders:\n",
    "        for data in dataloader:\n",
    "            adj = to_dense_adj(data.edge_index).squeeze(0)\n",
    "            graph = pygsp.graphs.Graph(adj.cpu().detach().numpy(), lap_type=lap_type)\n",
    "            graph.estimate_lmax()\n",
    "            tau = np.geomspace(tau_min, tau_max, n_tau)\n",
    "            n = adj.shape[0]\n",
    "            filt = pygsp.filters.Heat(graph, tau=tau)\n",
    "            heat_kernel = filt.filter(np.eye(n), order=order).reshape(\n",
    "                        n, n, -1\n",
    "                    )\n",
    "            heat_kernel[heat_kernel < 0] = 0\n",
    "            multi_geo = [\n",
    "                        np.sqrt(-4 * tau[i] * np.log(heat_kernel[:, :, i] + 1e-16)) \n",
    "                        for i in range(len(tau))\n",
    "                    ]\n",
    "            weights = 1 - tau / tau.sum()\n",
    "            w_t = weights.sum()\n",
    "            weights = weights / w_t if w_t > 0 else None\n",
    "            dist = torch.from_numpy(np.average(multi_geo, axis=0, weights=weights)).float()\n",
    "            dist = dist.sum(axis=1)\n",
    "            dist /= dist.max()\n",
    "            data.x = torch.hstack((data.x,dist[:,None]))\n",
    "            data.num_features = data.x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_heat_feature([train_loader,test_loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(7, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 006, Train Acc: 0.6533, Test Acc: 0.7368\n",
      "Epoch: 007, Train Acc: 0.7467, Test Acc: 0.7632\n",
      "Epoch: 008, Train Acc: 0.7267, Test Acc: 0.7632\n",
      "Epoch: 009, Train Acc: 0.7200, Test Acc: 0.7632\n",
      "Epoch: 010, Train Acc: 0.7133, Test Acc: 0.7895\n",
      "Epoch: 011, Train Acc: 0.7200, Test Acc: 0.7632\n",
      "Epoch: 012, Train Acc: 0.7200, Test Acc: 0.7895\n",
      "Epoch: 013, Train Acc: 0.7200, Test Acc: 0.7895\n",
      "Epoch: 014, Train Acc: 0.7133, Test Acc: 0.8421\n",
      "Epoch: 015, Train Acc: 0.7133, Test Acc: 0.8421\n",
      "Epoch: 016, Train Acc: 0.7533, Test Acc: 0.7368\n",
      "Epoch: 017, Train Acc: 0.7400, Test Acc: 0.7632\n",
      "Epoch: 018, Train Acc: 0.7133, Test Acc: 0.8421\n",
      "Epoch: 019, Train Acc: 0.7400, Test Acc: 0.7895\n",
      "Epoch: 020, Train Acc: 0.7533, Test Acc: 0.7368\n",
      "Epoch: 021, Train Acc: 0.7467, Test Acc: 0.7895\n",
      "Epoch: 022, Train Acc: 0.7467, Test Acc: 0.7895\n",
      "Epoch: 023, Train Acc: 0.7533, Test Acc: 0.7895\n",
      "Epoch: 024, Train Acc: 0.7267, Test Acc: 0.8421\n",
      "Epoch: 025, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 026, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 027, Train Acc: 0.7600, Test Acc: 0.8158\n",
      "Epoch: 028, Train Acc: 0.7533, Test Acc: 0.8421\n",
      "Epoch: 029, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 030, Train Acc: 0.7600, Test Acc: 0.8158\n",
      "Epoch: 031, Train Acc: 0.7600, Test Acc: 0.8158\n",
      "Epoch: 032, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 033, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 034, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 035, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 036, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 037, Train Acc: 0.7400, Test Acc: 0.7632\n",
      "Epoch: 038, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 039, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 040, Train Acc: 0.7533, Test Acc: 0.7368\n",
      "Epoch: 041, Train Acc: 0.7467, Test Acc: 0.7368\n",
      "Epoch: 042, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 043, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 044, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 045, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 046, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 047, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 048, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 049, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 050, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 051, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 052, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 053, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 054, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 055, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 056, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 057, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 058, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 059, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 060, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 061, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 062, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 063, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 064, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 065, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 066, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 067, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 068, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 069, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 070, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 071, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 072, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 073, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 074, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 075, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 076, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 077, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 078, Train Acc: 0.7733, Test Acc: 0.8421\n",
      "Epoch: 079, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 080, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 081, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 082, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 083, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 084, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 085, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 086, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 087, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 088, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 089, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 090, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 091, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 092, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 093, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 094, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 095, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 096, Train Acc: 0.7600, Test Acc: 0.7895\n",
      "Epoch: 097, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 098, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 099, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 100, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 101, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 102, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 103, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 104, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 105, Train Acc: 0.7733, Test Acc: 0.7368\n",
      "Epoch: 106, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 107, Train Acc: 0.7733, Test Acc: 0.7105\n",
      "Epoch: 108, Train Acc: 0.8000, Test Acc: 0.7632\n",
      "Epoch: 109, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 110, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 111, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 112, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 113, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 114, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 115, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 116, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 117, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 118, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 119, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 120, Train Acc: 0.8000, Test Acc: 0.7105\n",
      "Epoch: 121, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 122, Train Acc: 0.7667, Test Acc: 0.7105\n",
      "Epoch: 123, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 124, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 125, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 126, Train Acc: 0.7733, Test Acc: 0.7368\n",
      "Epoch: 127, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 128, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 129, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 130, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 131, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 132, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 133, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 134, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 135, Train Acc: 0.8067, Test Acc: 0.7368\n",
      "Epoch: 136, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 137, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 138, Train Acc: 0.8133, Test Acc: 0.7105\n",
      "Epoch: 139, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 140, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 141, Train Acc: 0.8000, Test Acc: 0.6579\n",
      "Epoch: 142, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 143, Train Acc: 0.7933, Test Acc: 0.7632\n",
      "Epoch: 144, Train Acc: 0.7867, Test Acc: 0.7368\n",
      "Epoch: 145, Train Acc: 0.8267, Test Acc: 0.7368\n",
      "Epoch: 146, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 147, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 148, Train Acc: 0.7933, Test Acc: 0.7895\n",
      "Epoch: 149, Train Acc: 0.8200, Test Acc: 0.7105\n",
      "Epoch: 150, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 151, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 152, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 153, Train Acc: 0.8067, Test Acc: 0.7368\n",
      "Epoch: 154, Train Acc: 0.8067, Test Acc: 0.7368\n",
      "Epoch: 155, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 156, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 157, Train Acc: 0.8000, Test Acc: 0.7368\n",
      "Epoch: 158, Train Acc: 0.7800, Test Acc: 0.7368\n",
      "Epoch: 159, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 160, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 161, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 162, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 163, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 164, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 165, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 166, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 167, Train Acc: 0.7867, Test Acc: 0.7895\n",
      "Epoch: 168, Train Acc: 0.7867, Test Acc: 0.7895\n",
      "Epoch: 169, Train Acc: 0.8000, Test Acc: 0.7632\n",
      "Epoch: 170, Train Acc: 0.8000, Test Acc: 0.7632\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'train_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/metric_emb/lib/python3.10/site-packages/torch_geometric/data/storage.py:62\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/metric_emb/lib/python3.10/site-packages/torch_geometric/data/storage.py:85\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mapping[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_mask'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/network/scratch/g/guillaume.huguet/projects/metric_embeddings/notebooks/4-gh-torch-geo-TUData.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-a006.server.mila.quebec/network/scratch/g/guillaume.huguet/projects/metric_embeddings/notebooks/4-gh-torch-geo-TUData.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m      \u001b[39mreturn\u001b[39;00m correct \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(loader\u001b[39m.\u001b[39mdataset)  \u001b[39m# Derive ratio of correct predictions.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-a006.server.mila.quebec/network/scratch/g/guillaume.huguet/projects/metric_embeddings/notebooks/4-gh-torch-geo-TUData.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcn-a006.server.mila.quebec/network/scratch/g/guillaume.huguet/projects/metric_embeddings/notebooks/4-gh-torch-geo-TUData.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-a006.server.mila.quebec/network/scratch/g/guillaume.huguet/projects/metric_embeddings/notebooks/4-gh-torch-geo-TUData.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     loss_class_train \u001b[39m=\u001b[39m train_class_loss()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-a006.server.mila.quebec/network/scratch/g/guillaume.huguet/projects/metric_embeddings/notebooks/4-gh-torch-geo-TUData.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/network/scratch/g/guillaume.huguet/projects/metric_embeddings/notebooks/4-gh-torch-geo-TUData.ipynb Cell 12\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcn-a006.server.mila.quebec/network/scratch/g/guillaume.huguet/projects/metric_embeddings/notebooks/4-gh-torch-geo-TUData.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcn-a006.server.mila.quebec/network/scratch/g/guillaume.huguet/projects/metric_embeddings/notebooks/4-gh-torch-geo-TUData.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m       train_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mix_(data\u001b[39m.\u001b[39;49mtrain_mask, data\u001b[39m.\u001b[39mtrain_mask)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcn-a006.server.mila.quebec/network/scratch/g/guillaume.huguet/projects/metric_embeddings/notebooks/4-gh-torch-geo-TUData.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m       model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcn-a006.server.mila.quebec/network/scratch/g/guillaume.huguet/projects/metric_embeddings/notebooks/4-gh-torch-geo-TUData.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m       optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Clear gradients.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/metric_emb/lib/python3.10/site-packages/torch_geometric/data/data.py:428\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_store\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m    423\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    424\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m object was created by an older version of PyG. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this error occurred while loading an already existing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset, remove the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mprocessed/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directory in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mroot folder and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 428\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store, key)\n",
      "File \u001b[0;32m~/.conda/envs/metric_emb/lib/python3.10/site-packages/torch_geometric/data/storage.py:64\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'train_mask'"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "# Trained to match heat-geodesic distances. \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train():\n",
    "      train_idx = np.ix_(data.train_mask, data.train_mask)\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      dist_pred = torch.cdist(out,out)\n",
    "      loss = criterion(dist_pred[train_idx], dist[train_idx])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = train()\n",
    "    loss_class_train = test()\n",
    "    if epoch % 5 == 0:\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {loss_class_train:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative heat-geodesic distance as node feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment feature with 0. \n",
    "zeros = torch.zeros((data.num_nodes,1))\n",
    "data.x = torch.hstack((data.x,zeros))\n",
    "data.num_features = data.x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(data.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Loss: 1.0662\n",
      "Epoch: 050, Loss test: 0.7750\n",
      "Epoch: 100, Loss: 0.5359\n",
      "Epoch: 100, Loss test: 0.7920\n",
      "Epoch: 150, Loss: 0.4030\n",
      "Epoch: 150, Loss test: 0.7970\n",
      "Epoch: 200, Loss: 0.3381\n",
      "Epoch: 200, Loss test: 0.7920\n",
      "Epoch: 250, Loss: 0.2531\n",
      "Epoch: 250, Loss test: 0.7870\n",
      "Epoch: 300, Loss: 0.2290\n",
      "Epoch: 300, Loss test: 0.7900\n",
      "Epoch: 350, Loss: 0.2317\n",
      "Epoch: 350, Loss test: 0.7870\n",
      "Epoch: 400, Loss: 0.2140\n",
      "Epoch: 400, Loss test: 0.7900\n",
      "Epoch: 450, Loss: 0.2206\n",
      "Epoch: 450, Loss test: 0.8020\n",
      "Epoch: 500, Loss: 0.1972\n",
      "Epoch: 500, Loss test: 0.8040\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc\n",
    "\n",
    "\n",
    "for epoch in range(1, 501):\n",
    "    loss = train()\n",
    "    if epoch % 50 ==0:\n",
    "      loss_test = test()\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "      print(f'Epoch: {epoch:03d}, Loss test: {loss_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/guillaume.huguet/.conda/envs/metric_emb/lib/python3.10/site-packages/graphtools/graphs.py:290: RuntimeWarning: Detected zero distance between 11 pairs of samples. Consider removing duplicates to avoid errors in downstream processing.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# with relative heat distance.\n",
    "data = dataset[0]\n",
    "emb_op = EmbHeatGeo(knn=10)\n",
    "emb_op.fit(data.x.detach().numpy())\n",
    "emb_op.metric_computation(data.x.detach().numpy(), n_tau=2)\n",
    "relative_dist = torch.from_numpy(emb_op.get_relative_dist()).unsqueeze(1).float()\n",
    "relative_dist /= relative_dist.max()\n",
    "data.x = torch.hstack((data.x,relative_dist))\n",
    "data.num_features = data.x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Loss: 1.2026\n",
      "Epoch: 050, Loss test: 0.7770\n",
      "Epoch: 100, Loss: 0.5699\n",
      "Epoch: 100, Loss test: 0.7970\n",
      "Epoch: 150, Loss: 0.4386\n",
      "Epoch: 150, Loss test: 0.7970\n",
      "Epoch: 200, Loss: 0.3837\n",
      "Epoch: 200, Loss test: 0.7930\n",
      "Epoch: 250, Loss: 0.3055\n",
      "Epoch: 250, Loss test: 0.7960\n",
      "Epoch: 300, Loss: 0.2686\n",
      "Epoch: 300, Loss test: 0.8050\n",
      "Epoch: 350, Loss: 0.2344\n",
      "Epoch: 350, Loss test: 0.7890\n",
      "Epoch: 400, Loss: 0.2276\n",
      "Epoch: 400, Loss test: 0.7950\n",
      "Epoch: 450, Loss: 0.2399\n",
      "Epoch: 450, Loss test: 0.8060\n",
      "Epoch: 500, Loss: 0.1981\n",
      "Epoch: 500, Loss test: 0.7990\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(1, 501):\n",
    "    loss = train()\n",
    "    if epoch % 50 ==0:\n",
    "      loss_test = test()\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "      print(f'Epoch: {epoch:03d}, Loss test: {loss_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metric_emb",
   "language": "python",
   "name": "metric_emb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a32b8ce1d8a7cc8940ec0fbf84ac7f0e06291324aa09cbfc1497fc150849df3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
